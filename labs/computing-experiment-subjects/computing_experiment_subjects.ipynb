{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Experiment Datasets #1: Experiment Subjects\n",
    "\n",
    "This Lab is part of a multi-part series focused on computing useful experiment datasets. In this Lab, we'll use [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) to compute _experiment subjects_ from [Optimizely Enriched Event](https://docs.developers.optimizely.com/optimizely-data/docs/enriched-events-export) [\"Decision\"](https://docs.developers.optimizely.com/optimizely-data/docs/enriched-events-data-specification#decisions-2) data.\n",
    "\n",
    "<!-- We use an external image URL rather than a relative path so that this notebook will be rendered correctly on the Optimizely Labs website -->\n",
    "![Experiment subjects computation](https://raw.githubusercontent.com/optimizely/labs/master/labs/computing-experiment-subjects/img/subjects_computation.png)\n",
    "\n",
    "**Experiment subjects** are the individual units that are exposed to a control or treatment in the course of an online experiment.  In most online experiments, subjects are website visitors or app users. However, depending on your experiment design, treatments may also be applied to individual user sessions, service requests, search queries, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this notebook\n",
    "\n",
    "This notebook lives in the [Optimizely Labs](http://github.com/optimizely/labs) repository.  You can download it and everything you need to run it by doing one of the following\n",
    "1. Downloading a zipped copy of this Lab directory on the [Optimizely Labs page](https://www.optimizely.com/labs/computing-experiment-subjects/)\n",
    "2. Downloading a [zipped copy of the Optimizely Labs repository](https://github.com/optimizely/labs/archive/master.zip) from Github\n",
    "3. Cloning the [Github respository](http://github.com/optimizely/labs)\n",
    "\n",
    "Once you've downloaded this Lab directory (on its own, or as part of the [Optimizely Labs](http://github.com/optimizely/labs) repository, follow the instructions in the `README.md` file for this Lab.\n",
    "\n",
    "If you wish to run this notebook using a custom data directory, set the `OPTIMIZELY_DATA_DIR` environment variable to point to the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "We'll use the following global variables to parameterize our computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Local data storage locations\n",
    "# These parameters specify where this notebook should read and write data. The default location\n",
    "# is ./example_data in this notebook's directory.  You can point the notebook to another data\n",
    "# directory by setting the OPTIMIZELY_DATA_DIR environment variable.\n",
    "base_data_dir = os.environ.get(\"OPTIMIZELY_DATA_DIR\", \"./example_data\")\n",
    "decisions_dir = os.path.join(base_data_dir, \"type=decisions\")\n",
    "subjects_output_dir = os.path.join(base_data_dir, \"type=subjects\")\n",
    "\n",
    "# Subject ID column\n",
    "# Used to group event-level records by experiment subject.  It's useful to modify this if you wish\n",
    "# use another identifier to denote an individual subject, such as\n",
    "#    1. \"session_id\" if you wish to compute session-level metrics with Optimizely data\n",
    "#    2. A custom ID field, perhaps one found in an the user attributes attached to a decision.\n",
    "#       Note that if you wish to group by a particular attribute, you'll need to add a step to \n",
    "#       transform your decision data such that this attribute is exposed in its own column\n",
    "#       https://docs.developers.optimizely.com/full-stack/docs/pass-in-audience-attributes-python\n",
    "subject_id = \"visitor_id\"\n",
    "\n",
    "# Analysis window\n",
    "# The analysis window determines which decisions are included in your analysis.  The default\n",
    "# window is this notebook is large enough that *all* decisions loaded will be included in \n",
    "# the computation, but you can adjust this to focus on a specific time window.\n",
    "#\n",
    "# Note that Optimizely Web and Optimizely Full Stack use slightly different attribution logic\n",
    "# to determine which subjects are counted in a particular time window.  The logic embedded in\n",
    "# the queries in this notebook mimics the Full Stack approach.  More on this below.\n",
    "decisions_start = \"2000-01-01 00:00:00\"\n",
    "decisions_end = \"2099-12-31 23:59:59\"\n",
    "\n",
    "# The following are useful for converting timestamps from Optimizely results page URLs into\n",
    "# an analysis window that can be used by the queries in this notebook.\n",
    "# decisions_start = datetime.fromtimestamp(1592416545427 / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "# decisions_end = datetime.fromtimestamp(1593108481887 / 1000).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.repl.eagerEval.truncate\", 120) \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load decision data\n",
    "\n",
    "We'll start by loading decision data and isolating the decisions for the experiment specified by `experiment_id` and the time window specfied by `decisions_start` and `decisions_end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>uuid</th><th>timestamp</th><th>process_timestamp</th><th>visitor_id</th><th>session_id</th><th>account_id</th><th>campaign_id</th><th>experiment_id</th><th>variation_id</th><th>attributes</th><th>user_ip</th><th>user_agent</th><th>referer</th><th>is_holdback</th><th>revision</th><th>client_engine</th><th>client_version</th></tr>\n",
       "<tr><td>F4F1EF48-6BC2-4153-A1DA-C29E39B772F9</td><td>2020-05-25 15:27:33.085</td><td>2020-05-25 15:29:21.197</td><td>visitor_1590445653085</td><td>-1235693267</td><td>596780373</td><td>18149940006</td><td>18156943409</td><td>18174970251</td><td>[[100,, browserId, ie], [300,, device, iphone], [600,, source_type, direct], [200,, campaign, frequent visitors], [, ...</td><td>75.111.77.0</td><td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/53...</td><td>https://app.optimizely.com/</td><td>false</td><td>null</td><td>ricky/fakedata.pwned</td><td>1.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------------------------+-----------------------+-----------------------+---------------------+-----------+----------+-----------+-------------+------------+------------------------------------------------------------------------------------------------------------------------+-----------+------------------------------------------------------------------------------------------------------------------------+---------------------------+-----------+--------+--------------------+--------------+\n",
       "|                                uuid|              timestamp|      process_timestamp|           visitor_id| session_id|account_id|campaign_id|experiment_id|variation_id|                                                                                                              attributes|    user_ip|                                                                                                              user_agent|                    referer|is_holdback|revision|       client_engine|client_version|\n",
       "+------------------------------------+-----------------------+-----------------------+---------------------+-----------+----------+-----------+-------------+------------+------------------------------------------------------------------------------------------------------------------------+-----------+------------------------------------------------------------------------------------------------------------------------+---------------------------+-----------+--------+--------------------+--------------+\n",
       "|F4F1EF48-6BC2-4153-A1DA-C29E39B772F9|2020-05-25 15:27:33.085|2020-05-25 15:29:21.197|visitor_1590445653085|-1235693267| 596780373|18149940006|  18156943409| 18174970251|[[100,, browserId, ie], [300,, device, iphone], [600,, source_type, direct], [200,, campaign, frequent visitors], [, ...|75.111.77.0|Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/53...|https://app.optimizely.com/|      false|    null|ricky/fakedata.pwned|         1.0.0|\n",
       "+------------------------------------+-----------------------+-----------------------+---------------------+-----------+----------+-----------+-------------+------------+------------------------------------------------------------------------------------------------------------------------+-----------+------------------------------------------------------------------------------------------------------------------------+---------------------------+-----------+--------+--------------------+--------------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(decisions_dir).createOrReplaceTempView(\"loaded_decisions\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW decisions as\n",
    "        SELECT \n",
    "           *\n",
    "        FROM \n",
    "            loaded_decisions\n",
    "        WHERE\n",
    "            timestamp BETWEEN '{decisions_start}' AND '{decisions_end}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM decisions LIMIT 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute experiment subjects\n",
    "\n",
    "Next we'll group our decision data by `decision_subject_id` in order to produce a table of _experiment subjects_.\n",
    "\n",
    "Experimental subjects are the randomization units used in our experiment. In most cases each subject corresponds to a web/app user, but they may also represent teams, user sessions, requests, etc., depending on the design of your experiment. In this example we use visitors (identified by the `visitor_id` field) as our experiment subjects.\n",
    "\n",
    "This query captures the attribution logic used by [Optimizely Full Stack](https://www.optimizely.com/platform/full-stack/) to isolate the visitors tested in a given experiment:\n",
    "- In order to be counted in the analysis, a visitor must have at least one \"decision\" in the analysis window.\n",
    "- The variation and the timestamp assigned to that visitor correspond to the variation in the _first_ decision received from that visitor during the analysis window\n",
    "\n",
    "**Note:** the visitor counting logic used in [Optimizely Web](https://www.optimizely.com/platform/experimentation/) experiments is slightly different.  See [this article](https://help.optimizely.com/Analyze_Results/How_Optimizely_counts_conversions) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>visitor_id</th><th>experiment_id</th><th>variation_id</th><th>timestamp</th></tr>\n",
       "<tr><td>visitor_1590445653085</td><td>18156943409</td><td>18174970251</td><td>2020-05-25 15:27:33.085</td></tr>\n",
       "<tr><td>visitor_1590445653325</td><td>18156943409</td><td>18112613000</td><td>2020-05-25 15:27:33.325</td></tr>\n",
       "<tr><td>visitor_1590445653565</td><td>18156943409</td><td>18112613000</td><td>2020-05-25 15:27:33.565</td></tr>\n",
       "<tr><td>visitor_1590445653805</td><td>18156943409</td><td>18174970251</td><td>2020-05-25 15:27:33.805</td></tr>\n",
       "<tr><td>visitor_1590445654045</td><td>18156943409</td><td>18174970251</td><td>2020-05-25 15:27:34.045</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------------+-------------+------------+-----------------------+\n",
       "|           visitor_id|experiment_id|variation_id|              timestamp|\n",
       "+---------------------+-------------+------------+-----------------------+\n",
       "|visitor_1590445653085|  18156943409| 18174970251|2020-05-25 15:27:33.085|\n",
       "|visitor_1590445653325|  18156943409| 18112613000|2020-05-25 15:27:33.325|\n",
       "|visitor_1590445653565|  18156943409| 18112613000|2020-05-25 15:27:33.565|\n",
       "|visitor_1590445653805|  18156943409| 18174970251|2020-05-25 15:27:33.805|\n",
       "|visitor_1590445654045|  18156943409| 18174970251|2020-05-25 15:27:34.045|\n",
       "+---------------------+-------------+------------+-----------------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW experiment_subjects AS\n",
    "        SELECT\n",
    "            {subject_id},\n",
    "            experiment_id,\n",
    "            variation_id,\n",
    "            timestamp\n",
    "        FROM (\n",
    "            SELECT\n",
    "                *,\n",
    "                RANK() OVER (PARTITION BY experiment_id, {decision_subject_id} ORDER BY timestamp ASC) AS rnk\n",
    "            FROM\n",
    "                decisions\n",
    "        )\n",
    "        WHERE\n",
    "            rnk = 1\n",
    "        ORDER BY timestamp ASC\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM experiment_subjects LIMIT 5\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing our subjects dataset to disk\n",
    "\n",
    "We'll store our experiment subjects in the directory specified by `subjects_output_dir`.  Subject data is partitioned into directories for each experiment included in the input decision data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM experiment_subjects\"\"\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .partitionBy(\"experiment_id\") \\\n",
    "    .parquet(subjects_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing our subject data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group by `experiment_id` and `variation_id` to count the number of subjects attributed to each variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>experiment_id</th><th>variation_id</th><th>subject_count</th></tr>\n",
       "<tr><td>18156943409</td><td>18112613000</td><td>4487</td></tr>\n",
       "<tr><td>18156943409</td><td>18174970251</td><td>4514</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+------------+-------------+\n",
       "|experiment_id|variation_id|subject_count|\n",
       "+-------------+------------+-------------+\n",
       "|  18156943409| 18112613000|         4487|\n",
       "|  18156943409| 18174970251|         4514|\n",
       "+-------------+------------+-------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        experiment_id, \n",
    "        variation_id, \n",
    "        COUNT(1) as subject_count \n",
    "    FROM \n",
    "        experiment_subjects \n",
    "    GROUP BY \n",
    "        experiment_id,\n",
    "        variation_id\n",
    "    ORDER BY\n",
    "        experiment_id,\n",
    "        variation_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
